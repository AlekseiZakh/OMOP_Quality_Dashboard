[tool:pytest]
# =============================================================================
# OMOP Quality Dashboard - Pytest Configuration
# =============================================================================

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_* *_test

# Basic test execution options
addopts = 
    -v
    --tb=short
    --strict-markers
    --strict-config
    --disable-warnings
    --show-capture=no
    --maxfail=10
    --durations=10
    --color=yes
    
    # Coverage configuration
    --cov=app
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-report=term-missing
    --cov-fail-under=80
    --cov-branch
    
    # Performance and timing
    --benchmark-disable
    --timeout=300
    
    # Parallel execution (uncomment if pytest-xdist is installed)
    # -n auto
    
    # Distributed testing options
    # --dist=loadscope

# Minimum version requirements
minversion = 6.0

# Test timeout (in seconds)
timeout = 300
timeout_method = thread

# Logging configuration
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Automatically capture logs from specific loggers
log_auto_indent = true

# Filter out specific warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*imp module.*:DeprecationWarning
    ignore:.*pkg_resources.*:DeprecationWarning
    ignore:.*distutils.*:DeprecationWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    ignore:.*streamlit.*:UserWarning
    ignore:.*plotly.*:UserWarning
    ignore:.*pandas.*:FutureWarning
    error::UserWarning:app.*

# Test markers - organized by category
markers =
    # Test Types
    unit: Unit tests - fast, isolated tests
    integration: Integration tests - test component interactions
    e2e: End-to-end tests - full application workflow tests
    smoke: Smoke tests - basic functionality verification
    regression: Regression tests - prevent known bugs from reoccurring
    
    # Performance and Load
    slow: Slow running tests (> 5 seconds)
    performance: Performance benchmark tests
    memory: Memory usage tests
    load: Load testing with large datasets
    
    # Components
    database: Tests requiring database connection
    ui: Tests for UI components and Streamlit interface
    api: Tests for API endpoints (future)
    cli: Tests for command line interface
    
    # Quality Check Categories
    completeness: Tests for data completeness checks
    temporal: Tests for temporal consistency checks
    concept_mapping: Tests for concept mapping quality
    referential: Tests for referential integrity checks
    statistical: Tests for statistical outlier detection
    
    # Infrastructure
    docker: Tests for Docker functionality
    security: Security-related tests
    config: Configuration and settings tests
    export: Data export functionality tests
    
    # Data Categories
    real_data: Tests using real/production-like data
    mock_data: Tests using mock/synthetic data
    empty_data: Tests with empty datasets
    large_data: Tests with large datasets (>10k records)
    
    # External Dependencies
    external_db: Tests requiring external database
    network: Tests requiring network access
    file_system: Tests requiring file system access
    
    # Test Quality
    flaky: Tests that may fail intermittently
    skip_ci: Tests to skip in CI environment
    manual: Tests requiring manual intervention
    
    # Environment Specific
    dev: Development environment only
    staging: Staging environment tests
    production: Production environment tests
    
    # Browser/Platform Specific (future)
    chrome: Chrome browser tests
    firefox: Firefox browser tests
    edge: Edge browser tests
    mobile: Mobile interface tests

# Coverage configuration
[coverage:run]
source = app
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */site-packages/*
    */venv/*
    */env/*
    app/main.py
    setup.py
    conftest.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

precision = 2
show_missing = true
skip_covered = false
skip_empty = false

[coverage:html]
directory = htmlcov
title = OMOP Quality Dashboard Test Coverage

[coverage:xml]
output = coverage.xml

# Pytest-benchmark configuration
[tool:pytest-benchmark]
min_rounds = 5
max_time = 1.0
min_time = 0.000005
timer = time.perf_counter
calibration_precision = 10
warmup = false
warmup_iterations = 100000
disable_gc = false
sort = min
histogram = true
save = .benchmarks
save_data = true
autosave = true
compare_fail = mean:5%
compare_skip = 0.01

# Pytest-xdist configuration (for parallel testing)
[tool:pytest-xdist]
looponfailroots = app tests

# Pytest-mock configuration
[tool:pytest-mock]
mock_use_standalone_module = true

# Test data configuration
[tool:pytest-testmon]
testmon_off = false

# Pytest-html configuration (for HTML reports)
[tool:pytest-html]
self_contained_html = true

# Custom test configuration sections
[omop:test_data]
# Test data configuration
small_dataset_size = 100
medium_dataset_size = 1000
large_dataset_size = 10000
test_database_url = sqlite:///test_omop.db
sample_data_path = tests/fixtures/

[omop:quality_thresholds]
# Quality check thresholds for testing
min_completeness_score = 80
max_acceptable_future_dates = 0
max_unmapped_percentage = 5
max_outlier_count = 10

[omop:performance_limits]
# Performance limits for tests
max_query_time_seconds = 30
max_memory_mb = 500
max_dashboard_load_time = 10

[omop:test_categories]
# Test category execution times
unit_max_time = 1
integration_max_time = 30
e2e_max_time = 300
performance_max_time = 600
